Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.96s/it]

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 8, 1][8, 1, 1]cuda:0", L_fn_self_streaming_state_cache: "i64[1, 17, 21][357, 21, 1]cuda:0", L_fn_self_streaming_state_initial: "i64[1, 17, 1][17, 1, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_fn_self_streaming_state_cache = L_fn_self_streaming_state_cache
        l_fn_self_streaming_state_initial = L_fn_self_streaming_state_initial
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 9, slice(0, 1, None))] = getitem;  setitem = l_fn_self_streaming_state_cache;  getitem = setitem = None
        getitem_1: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 1)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 10, slice(1, 2, None))] = getitem_1;  setitem_1 = l_fn_self_streaming_state_cache;  getitem_1 = setitem_1 = None
        getitem_2: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 2)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 11, slice(1, 2, None))] = getitem_2;  setitem_2 = l_fn_self_streaming_state_cache;  getitem_2 = setitem_2 = None
        getitem_3: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 3)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 12, slice(1, 2, None))] = getitem_3;  setitem_3 = l_fn_self_streaming_state_cache;  getitem_3 = setitem_3 = None
        getitem_4: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 4)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 13, slice(1, 2, None))] = getitem_4;  setitem_4 = l_fn_self_streaming_state_cache;  getitem_4 = setitem_4 = None
        getitem_5: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 5)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 14, slice(1, 2, None))] = getitem_5;  setitem_5 = l_fn_self_streaming_state_cache;  getitem_5 = setitem_5 = None
        getitem_6: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 6)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 15, slice(1, 2, None))] = getitem_6;  setitem_6 = l_fn_self_streaming_state_cache;  getitem_6 = setitem_6 = None
        getitem_7: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 7)];  l_args_0_ = None
        l_fn_self_streaming_state_cache[(slice(None, None, None), 16, slice(1, 2, None))] = getitem_7;  setitem_7 = l_fn_self_streaming_state_cache;  getitem_7 = setitem_7 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:118 in step, code: state.cache[:, k, position] = state.initial[:, k, 0]
        getitem_8: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 0, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 0, 0)] = getitem_8;  setitem_8 = l_fn_self_streaming_state_cache;  getitem_8 = setitem_8 = None
        getitem_9: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 1, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 1, 0)] = getitem_9;  setitem_9 = l_fn_self_streaming_state_cache;  getitem_9 = setitem_9 = None
        getitem_10: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 2, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 2, 0)] = getitem_10;  setitem_10 = l_fn_self_streaming_state_cache;  getitem_10 = setitem_10 = None
        getitem_11: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 3, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 3, 0)] = getitem_11;  setitem_11 = l_fn_self_streaming_state_cache;  getitem_11 = setitem_11 = None
        getitem_12: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 4, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 4, 0)] = getitem_12;  setitem_12 = l_fn_self_streaming_state_cache;  getitem_12 = setitem_12 = None
        getitem_13: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 5, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 5, 0)] = getitem_13;  setitem_13 = l_fn_self_streaming_state_cache;  getitem_13 = setitem_13 = None
        getitem_14: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 6, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 6, 0)] = getitem_14;  setitem_14 = l_fn_self_streaming_state_cache;  getitem_14 = setitem_14 = None
        getitem_15: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 7, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 7, 0)] = getitem_15;  setitem_15 = l_fn_self_streaming_state_cache;  getitem_15 = setitem_15 = None
        getitem_16: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 8, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 8, 0)] = getitem_16;  setitem_16 = l_fn_self_streaming_state_cache;  getitem_16 = setitem_16 = None
        getitem_17: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 10, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 10, 0)] = getitem_17;  setitem_17 = l_fn_self_streaming_state_cache;  getitem_17 = setitem_17 = None
        getitem_18: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 11, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 11, 0)] = getitem_18;  setitem_18 = l_fn_self_streaming_state_cache;  getitem_18 = setitem_18 = None
        getitem_19: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 12, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 12, 0)] = getitem_19;  setitem_19 = l_fn_self_streaming_state_cache;  getitem_19 = setitem_19 = None
        getitem_20: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 13, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 13, 0)] = getitem_20;  setitem_20 = l_fn_self_streaming_state_cache;  getitem_20 = setitem_20 = None
        getitem_21: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 14, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 14, 0)] = getitem_21;  setitem_21 = l_fn_self_streaming_state_cache;  getitem_21 = setitem_21 = None
        getitem_22: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 15, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 15, 0)] = getitem_22;  setitem_22 = l_fn_self_streaming_state_cache;  getitem_22 = setitem_22 = None
        getitem_23: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 16, 0)];  l_fn_self_streaming_state_initial = None
        l_fn_self_streaming_state_cache[(slice(None, None, None), 16, 0)] = getitem_23;  setitem_23 = l_fn_self_streaming_state_cache;  getitem_23 = setitem_23 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:119 in step, code: input_ = state.cache[:, :, position : position + 1]
        input_: "i64[1, 17, 1][357, 21, 1]cuda:0" = l_fn_self_streaming_state_cache[(slice(None, None, None), slice(None, None, None), slice(0, 1, None))];  l_fn_self_streaming_state_cache = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:120 in step, code: input_ = input_.permute(0, 2, 1).contiguous()
        permute: "i64[1, 1, 17][357, 1, 21]cuda:0" = input_.permute(0, 2, 1);  input_ = None
        input__1: "i64[1, 1, 17][17, 17, 1]cuda:0" = permute.contiguous();  permute = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:122 in step, code: 'input_ids': input_[:, :, 0],
        getitem_25: "i64[1, 1][17, 17]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), 0)]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:123 in step, code: 'speak_ids': input_[:, :, 1:9],
        getitem_26: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(1, 9, None))]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:124 in step, code: 'listen_ids': input_[:, :, 9:],
        getitem_27: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(9, None, None))];  input__1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_25, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq = getitem_25 == None;  getitem_25 = eq = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_28: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_28, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_28 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_29: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_29, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_29 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_30: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_30 += add;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_30;  getitem_30 = add = None
        embeddings[-1] = iadd;  setitem_24 = embeddings;  iadd = setitem_24 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_31: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_31, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_31 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_32: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_32, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_32 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_33: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_1: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_33 += add_1;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_33;  getitem_33 = add_1 = None
        embeddings[-1] = iadd_1;  setitem_25 = embeddings;  iadd_1 = setitem_25 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_34: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_34, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_34 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_35: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_35, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_35 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_36: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_2: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_36 += add_2;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_36;  getitem_36 = add_2 = None
        embeddings[-1] = iadd_2;  setitem_26 = embeddings;  iadd_2 = setitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_37: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_37, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_37 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_38: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_38, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_38 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_39: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_3: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_39 += add_3;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_39;  getitem_39 = add_3 = None
        embeddings[-1] = iadd_3;  setitem_27 = embeddings;  iadd_3 = setitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_40: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_40, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_40 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_41: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_41, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_41 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_42: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_4: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_42 += add_4;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_42;  getitem_42 = add_4 = None
        embeddings[-1] = iadd_4;  setitem_28 = embeddings;  iadd_4 = setitem_28 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_43: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_43, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_43 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_44: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_44, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_44 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_45: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_5: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_45 += add_5;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_45;  getitem_45 = add_5 = None
        embeddings[-1] = iadd_5;  setitem_29 = embeddings;  iadd_5 = setitem_29 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_46: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_46, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_46 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_47: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_47, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_47 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_48: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_6: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_48 += add_6;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_48;  getitem_48 = add_6 = None
        embeddings[-1] = iadd_6;  setitem_30 = embeddings;  iadd_6 = setitem_30 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_49: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 7)];  getitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_49, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_49 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_50: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 7)];  getitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_50, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_50 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_51: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_7: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_51 += add_7;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_51;  getitem_51 = add_7 = None
        embeddings[-1] = iadd_7;  setitem_31 = embeddings;  iadd_7 = setitem_31 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(0, 1, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_52: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_1: "b8[][]cuda:0" = getitem_52 == 0;  getitem_52 = eq_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 8, 1][8, 1, 1]cuda:0", L_fn_self_streaming_state_cache: "i64[1, 17, 21][357, 21, 1]cuda:0", L_fn_self_streaming_state_initial: "i64[1, 17, 1][17, 1, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_fn_self_streaming_state_cache = L_fn_self_streaming_state_cache
        l_fn_self_streaming_state_initial = L_fn_self_streaming_state_initial
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 9, slice(0, 1, None))] = getitem;  setitem = l_fn_self_streaming_state_cache;  getitem = setitem = None
        getitem_1: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 1)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 10, slice(1, 2, None))] = getitem_1;  setitem_1 = l_fn_self_streaming_state_cache;  getitem_1 = setitem_1 = None
        getitem_2: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 2)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 11, slice(1, 2, None))] = getitem_2;  setitem_2 = l_fn_self_streaming_state_cache;  getitem_2 = setitem_2 = None
        getitem_3: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 3)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 12, slice(1, 2, None))] = getitem_3;  setitem_3 = l_fn_self_streaming_state_cache;  getitem_3 = setitem_3 = None
        getitem_4: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 4)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 13, slice(1, 2, None))] = getitem_4;  setitem_4 = l_fn_self_streaming_state_cache;  getitem_4 = setitem_4 = None
        getitem_5: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 5)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 14, slice(1, 2, None))] = getitem_5;  setitem_5 = l_fn_self_streaming_state_cache;  getitem_5 = setitem_5 = None
        getitem_6: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 6)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 15, slice(1, 2, None))] = getitem_6;  setitem_6 = l_fn_self_streaming_state_cache;  getitem_6 = setitem_6 = None
        getitem_7: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 7)];  l_args_0_ = None
        l_fn_self_streaming_state_cache[(slice(None, None, None), 16, slice(1, 2, None))] = getitem_7;  setitem_7 = l_fn_self_streaming_state_cache;  getitem_7 = setitem_7 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:118 in step, code: state.cache[:, k, position] = state.initial[:, k, 0]
        getitem_8: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 0, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 0, 0)] = getitem_8;  setitem_8 = l_fn_self_streaming_state_cache;  getitem_8 = setitem_8 = None
        getitem_9: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 1, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 1, 0)] = getitem_9;  setitem_9 = l_fn_self_streaming_state_cache;  getitem_9 = setitem_9 = None
        getitem_10: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 2, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 2, 0)] = getitem_10;  setitem_10 = l_fn_self_streaming_state_cache;  getitem_10 = setitem_10 = None
        getitem_11: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 3, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 3, 0)] = getitem_11;  setitem_11 = l_fn_self_streaming_state_cache;  getitem_11 = setitem_11 = None
        getitem_12: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 4, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 4, 0)] = getitem_12;  setitem_12 = l_fn_self_streaming_state_cache;  getitem_12 = setitem_12 = None
        getitem_13: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 5, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 5, 0)] = getitem_13;  setitem_13 = l_fn_self_streaming_state_cache;  getitem_13 = setitem_13 = None
        getitem_14: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 6, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 6, 0)] = getitem_14;  setitem_14 = l_fn_self_streaming_state_cache;  getitem_14 = setitem_14 = None
        getitem_15: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 7, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 7, 0)] = getitem_15;  setitem_15 = l_fn_self_streaming_state_cache;  getitem_15 = setitem_15 = None
        getitem_16: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 8, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 8, 0)] = getitem_16;  setitem_16 = l_fn_self_streaming_state_cache;  getitem_16 = setitem_16 = None
        getitem_17: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 10, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 10, 0)] = getitem_17;  setitem_17 = l_fn_self_streaming_state_cache;  getitem_17 = setitem_17 = None
        getitem_18: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 11, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 11, 0)] = getitem_18;  setitem_18 = l_fn_self_streaming_state_cache;  getitem_18 = setitem_18 = None
        getitem_19: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 12, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 12, 0)] = getitem_19;  setitem_19 = l_fn_self_streaming_state_cache;  getitem_19 = setitem_19 = None
        getitem_20: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 13, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 13, 0)] = getitem_20;  setitem_20 = l_fn_self_streaming_state_cache;  getitem_20 = setitem_20 = None
        getitem_21: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 14, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 14, 0)] = getitem_21;  setitem_21 = l_fn_self_streaming_state_cache;  getitem_21 = setitem_21 = None
        getitem_22: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 15, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 15, 0)] = getitem_22;  setitem_22 = l_fn_self_streaming_state_cache;  getitem_22 = setitem_22 = None
        getitem_23: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 16, 0)];  l_fn_self_streaming_state_initial = None
        l_fn_self_streaming_state_cache[(slice(None, None, None), 16, 0)] = getitem_23;  setitem_23 = l_fn_self_streaming_state_cache;  getitem_23 = setitem_23 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:119 in step, code: input_ = state.cache[:, :, position : position + 1]
        input_: "i64[1, 17, 1][357, 21, 1]cuda:0" = l_fn_self_streaming_state_cache[(slice(None, None, None), slice(None, None, None), slice(0, 1, None))];  l_fn_self_streaming_state_cache = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:120 in step, code: input_ = input_.permute(0, 2, 1).contiguous()
        permute: "i64[1, 1, 17][357, 1, 21]cuda:0" = input_.permute(0, 2, 1);  input_ = None
        input__1: "i64[1, 1, 17][17, 17, 1]cuda:0" = permute.contiguous();  permute = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:122 in step, code: 'input_ids': input_[:, :, 0],
        getitem_25: "i64[1, 1][17, 17]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), 0)]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:123 in step, code: 'speak_ids': input_[:, :, 1:9],
        getitem_26: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(1, 9, None))]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:124 in step, code: 'listen_ids': input_[:, :, 9:],
        getitem_27: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(9, None, None))];  input__1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_25, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq = getitem_25 == None;  getitem_25 = eq = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_28: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_28, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_28 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_29: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_29, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_29 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_30: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_30 += add;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_30;  getitem_30 = add = None
        embeddings[-1] = iadd;  setitem_24 = embeddings;  iadd = setitem_24 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_31: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_31, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_31 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_32: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_32, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_32 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_33: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_1: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_33 += add_1;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_33;  getitem_33 = add_1 = None
        embeddings[-1] = iadd_1;  setitem_25 = embeddings;  iadd_1 = setitem_25 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_34: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_34, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_34 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_35: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_35, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_35 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_36: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_2: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_36 += add_2;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_36;  getitem_36 = add_2 = None
        embeddings[-1] = iadd_2;  setitem_26 = embeddings;  iadd_2 = setitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_37: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_37, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_37 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_38: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_38, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_38 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_39: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_3: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_39 += add_3;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_39;  getitem_39 = add_3 = None
        embeddings[-1] = iadd_3;  setitem_27 = embeddings;  iadd_3 = setitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_40: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_40, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_40 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_41: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_41, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_41 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_42: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_4: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_42 += add_4;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_42;  getitem_42 = add_4 = None
        embeddings[-1] = iadd_4;  setitem_28 = embeddings;  iadd_4 = setitem_28 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_43: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_43, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_43 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_44: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_44, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_44 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_45: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_5: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_45 += add_5;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_45;  getitem_45 = add_5 = None
        embeddings[-1] = iadd_5;  setitem_29 = embeddings;  iadd_5 = setitem_29 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_46: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_46, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_46 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_47: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_47, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_47 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_48: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_6: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_48 += add_6;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_48;  getitem_48 = add_6 = None
        embeddings[-1] = iadd_6;  setitem_30 = embeddings;  iadd_6 = setitem_30 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_49: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 7)];  getitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_49, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_49 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_50: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 7)];  getitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_50, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_50 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_51: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_7: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_51 += add_7;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_51;  getitem_51 = add_7 = None
        embeddings[-1] = iadd_7;  setitem_31 = embeddings;  iadd_7 = setitem_31 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(0, 1, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_52: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_1: "b8[][]cuda:0" = getitem_52 == 0;  getitem_52 = eq_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 8, 1][8, 1, 1]cuda:0", L_fn_self_streaming_state_cache: "i64[1, 17, 21][357, 21, 1]cuda:0", L_fn_self_streaming_state_initial: "i64[1, 17, 1][17, 1, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_fn_self_streaming_state_cache = L_fn_self_streaming_state_cache
        l_fn_self_streaming_state_initial = L_fn_self_streaming_state_initial
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 9, slice(0, 1, None))] = getitem;  setitem = l_fn_self_streaming_state_cache;  getitem = setitem = None
        getitem_1: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 1)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 10, slice(1, 2, None))] = getitem_1;  setitem_1 = l_fn_self_streaming_state_cache;  getitem_1 = setitem_1 = None
        getitem_2: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 2)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 11, slice(1, 2, None))] = getitem_2;  setitem_2 = l_fn_self_streaming_state_cache;  getitem_2 = setitem_2 = None
        getitem_3: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 3)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 12, slice(1, 2, None))] = getitem_3;  setitem_3 = l_fn_self_streaming_state_cache;  getitem_3 = setitem_3 = None
        getitem_4: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 4)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 13, slice(1, 2, None))] = getitem_4;  setitem_4 = l_fn_self_streaming_state_cache;  getitem_4 = setitem_4 = None
        getitem_5: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 5)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 14, slice(1, 2, None))] = getitem_5;  setitem_5 = l_fn_self_streaming_state_cache;  getitem_5 = setitem_5 = None
        getitem_6: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 6)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 15, slice(1, 2, None))] = getitem_6;  setitem_6 = l_fn_self_streaming_state_cache;  getitem_6 = setitem_6 = None
        getitem_7: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 7)];  l_args_0_ = None
        l_fn_self_streaming_state_cache[(slice(None, None, None), 16, slice(1, 2, None))] = getitem_7;  setitem_7 = l_fn_self_streaming_state_cache;  getitem_7 = setitem_7 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:118 in step, code: state.cache[:, k, position] = state.initial[:, k, 0]
        getitem_8: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 0, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 0, 0)] = getitem_8;  setitem_8 = l_fn_self_streaming_state_cache;  getitem_8 = setitem_8 = None
        getitem_9: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 1, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 1, 0)] = getitem_9;  setitem_9 = l_fn_self_streaming_state_cache;  getitem_9 = setitem_9 = None
        getitem_10: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 2, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 2, 0)] = getitem_10;  setitem_10 = l_fn_self_streaming_state_cache;  getitem_10 = setitem_10 = None
        getitem_11: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 3, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 3, 0)] = getitem_11;  setitem_11 = l_fn_self_streaming_state_cache;  getitem_11 = setitem_11 = None
        getitem_12: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 4, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 4, 0)] = getitem_12;  setitem_12 = l_fn_self_streaming_state_cache;  getitem_12 = setitem_12 = None
        getitem_13: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 5, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 5, 0)] = getitem_13;  setitem_13 = l_fn_self_streaming_state_cache;  getitem_13 = setitem_13 = None
        getitem_14: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 6, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 6, 0)] = getitem_14;  setitem_14 = l_fn_self_streaming_state_cache;  getitem_14 = setitem_14 = None
        getitem_15: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 7, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 7, 0)] = getitem_15;  setitem_15 = l_fn_self_streaming_state_cache;  getitem_15 = setitem_15 = None
        getitem_16: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 8, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 8, 0)] = getitem_16;  setitem_16 = l_fn_self_streaming_state_cache;  getitem_16 = setitem_16 = None
        getitem_17: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 10, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 10, 0)] = getitem_17;  setitem_17 = l_fn_self_streaming_state_cache;  getitem_17 = setitem_17 = None
        getitem_18: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 11, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 11, 0)] = getitem_18;  setitem_18 = l_fn_self_streaming_state_cache;  getitem_18 = setitem_18 = None
        getitem_19: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 12, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 12, 0)] = getitem_19;  setitem_19 = l_fn_self_streaming_state_cache;  getitem_19 = setitem_19 = None
        getitem_20: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 13, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 13, 0)] = getitem_20;  setitem_20 = l_fn_self_streaming_state_cache;  getitem_20 = setitem_20 = None
        getitem_21: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 14, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 14, 0)] = getitem_21;  setitem_21 = l_fn_self_streaming_state_cache;  getitem_21 = setitem_21 = None
        getitem_22: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 15, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 15, 0)] = getitem_22;  setitem_22 = l_fn_self_streaming_state_cache;  getitem_22 = setitem_22 = None
        getitem_23: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 16, 0)];  l_fn_self_streaming_state_initial = None
        l_fn_self_streaming_state_cache[(slice(None, None, None), 16, 0)] = getitem_23;  setitem_23 = l_fn_self_streaming_state_cache;  getitem_23 = setitem_23 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:119 in step, code: input_ = state.cache[:, :, position : position + 1]
        input_: "i64[1, 17, 1][357, 21, 1]cuda:0" = l_fn_self_streaming_state_cache[(slice(None, None, None), slice(None, None, None), slice(0, 1, None))];  l_fn_self_streaming_state_cache = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:120 in step, code: input_ = input_.permute(0, 2, 1).contiguous()
        permute: "i64[1, 1, 17][357, 1, 21]cuda:0" = input_.permute(0, 2, 1);  input_ = None
        input__1: "i64[1, 1, 17][17, 17, 1]cuda:0" = permute.contiguous();  permute = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:122 in step, code: 'input_ids': input_[:, :, 0],
        getitem_25: "i64[1, 1][17, 17]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), 0)]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:123 in step, code: 'speak_ids': input_[:, :, 1:9],
        getitem_26: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(1, 9, None))]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:124 in step, code: 'listen_ids': input_[:, :, 9:],
        getitem_27: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(9, None, None))];  input__1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_25, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq = getitem_25 == None;  getitem_25 = eq = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_28: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_28, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_28 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_29: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_29, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_29 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_30: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_30 += add;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_30;  getitem_30 = add = None
        embeddings[-1] = iadd;  setitem_24 = embeddings;  iadd = setitem_24 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_31: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_31, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_31 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_32: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_32, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_32 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_33: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_1: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_33 += add_1;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_33;  getitem_33 = add_1 = None
        embeddings[-1] = iadd_1;  setitem_25 = embeddings;  iadd_1 = setitem_25 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_34: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_34, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_34 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_35: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_35, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_35 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_36: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_2: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_36 += add_2;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_36;  getitem_36 = add_2 = None
        embeddings[-1] = iadd_2;  setitem_26 = embeddings;  iadd_2 = setitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_37: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_37, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_37 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_38: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_38, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_38 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_39: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_3: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_39 += add_3;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_39;  getitem_39 = add_3 = None
        embeddings[-1] = iadd_3;  setitem_27 = embeddings;  iadd_3 = setitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_40: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_40, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_40 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_41: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_41, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_41 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_42: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_4: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_42 += add_4;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_42;  getitem_42 = add_4 = None
        embeddings[-1] = iadd_4;  setitem_28 = embeddings;  iadd_4 = setitem_28 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_43: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_43, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_43 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_44: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_44, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_44 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_45: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_5: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_45 += add_5;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_45;  getitem_45 = add_5 = None
        embeddings[-1] = iadd_5;  setitem_29 = embeddings;  iadd_5 = setitem_29 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_46: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_46, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_46 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_47: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_47, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_47 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_48: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_6: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_48 += add_6;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_48;  getitem_48 = add_6 = None
        embeddings[-1] = iadd_6;  setitem_30 = embeddings;  iadd_6 = setitem_30 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_49: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 7)];  getitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_49, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_49 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_50: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 7)];  getitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_50, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_50 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_51: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_7: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_51 += add_7;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_51;  getitem_51 = add_7 = None
        embeddings[-1] = iadd_7;  setitem_31 = embeddings;  iadd_7 = setitem_31 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(0, 1, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_52: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_1: "b8[][]cuda:0" = getitem_52 == 0;  getitem_52 = eq_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 8, 1][8, 1, 1]cuda:0", L_fn_self_streaming_state_cache: "i64[1, 17, 21][357, 21, 1]cuda:0", L_fn_self_streaming_state_initial: "i64[1, 17, 1][17, 1, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_fn_self_streaming_state_cache = L_fn_self_streaming_state_cache
        l_fn_self_streaming_state_initial = L_fn_self_streaming_state_initial
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 9, slice(0, 1, None))] = getitem;  setitem = l_fn_self_streaming_state_cache;  getitem = setitem = None
        getitem_1: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 1)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 10, slice(1, 2, None))] = getitem_1;  setitem_1 = l_fn_self_streaming_state_cache;  getitem_1 = setitem_1 = None
        getitem_2: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 2)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 11, slice(1, 2, None))] = getitem_2;  setitem_2 = l_fn_self_streaming_state_cache;  getitem_2 = setitem_2 = None
        getitem_3: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 3)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 12, slice(1, 2, None))] = getitem_3;  setitem_3 = l_fn_self_streaming_state_cache;  getitem_3 = setitem_3 = None
        getitem_4: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 4)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 13, slice(1, 2, None))] = getitem_4;  setitem_4 = l_fn_self_streaming_state_cache;  getitem_4 = setitem_4 = None
        getitem_5: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 5)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 14, slice(1, 2, None))] = getitem_5;  setitem_5 = l_fn_self_streaming_state_cache;  getitem_5 = setitem_5 = None
        getitem_6: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 6)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 15, slice(1, 2, None))] = getitem_6;  setitem_6 = l_fn_self_streaming_state_cache;  getitem_6 = setitem_6 = None
        getitem_7: "i64[1, 1][8, 1]cuda:0" = l_args_0_[(slice(None, None, None), 7)];  l_args_0_ = None
        l_fn_self_streaming_state_cache[(slice(None, None, None), 16, slice(1, 2, None))] = getitem_7;  setitem_7 = l_fn_self_streaming_state_cache;  getitem_7 = setitem_7 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:118 in step, code: state.cache[:, k, position] = state.initial[:, k, 0]
        getitem_8: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 0, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 0, 0)] = getitem_8;  setitem_8 = l_fn_self_streaming_state_cache;  getitem_8 = setitem_8 = None
        getitem_9: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 1, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 1, 0)] = getitem_9;  setitem_9 = l_fn_self_streaming_state_cache;  getitem_9 = setitem_9 = None
        getitem_10: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 2, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 2, 0)] = getitem_10;  setitem_10 = l_fn_self_streaming_state_cache;  getitem_10 = setitem_10 = None
        getitem_11: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 3, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 3, 0)] = getitem_11;  setitem_11 = l_fn_self_streaming_state_cache;  getitem_11 = setitem_11 = None
        getitem_12: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 4, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 4, 0)] = getitem_12;  setitem_12 = l_fn_self_streaming_state_cache;  getitem_12 = setitem_12 = None
        getitem_13: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 5, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 5, 0)] = getitem_13;  setitem_13 = l_fn_self_streaming_state_cache;  getitem_13 = setitem_13 = None
        getitem_14: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 6, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 6, 0)] = getitem_14;  setitem_14 = l_fn_self_streaming_state_cache;  getitem_14 = setitem_14 = None
        getitem_15: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 7, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 7, 0)] = getitem_15;  setitem_15 = l_fn_self_streaming_state_cache;  getitem_15 = setitem_15 = None
        getitem_16: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 8, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 8, 0)] = getitem_16;  setitem_16 = l_fn_self_streaming_state_cache;  getitem_16 = setitem_16 = None
        getitem_17: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 10, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 10, 0)] = getitem_17;  setitem_17 = l_fn_self_streaming_state_cache;  getitem_17 = setitem_17 = None
        getitem_18: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 11, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 11, 0)] = getitem_18;  setitem_18 = l_fn_self_streaming_state_cache;  getitem_18 = setitem_18 = None
        getitem_19: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 12, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 12, 0)] = getitem_19;  setitem_19 = l_fn_self_streaming_state_cache;  getitem_19 = setitem_19 = None
        getitem_20: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 13, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 13, 0)] = getitem_20;  setitem_20 = l_fn_self_streaming_state_cache;  getitem_20 = setitem_20 = None
        getitem_21: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 14, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 14, 0)] = getitem_21;  setitem_21 = l_fn_self_streaming_state_cache;  getitem_21 = setitem_21 = None
        getitem_22: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 15, 0)]
        l_fn_self_streaming_state_cache[(slice(None, None, None), 15, 0)] = getitem_22;  setitem_22 = l_fn_self_streaming_state_cache;  getitem_22 = setitem_22 = None
        getitem_23: "i64[1][17]cuda:0" = l_fn_self_streaming_state_initial[(slice(None, None, None), 16, 0)];  l_fn_self_streaming_state_initial = None
        l_fn_self_streaming_state_cache[(slice(None, None, None), 16, 0)] = getitem_23;  setitem_23 = l_fn_self_streaming_state_cache;  getitem_23 = setitem_23 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:119 in step, code: input_ = state.cache[:, :, position : position + 1]
        input_: "i64[1, 17, 1][357, 21, 1]cuda:0" = l_fn_self_streaming_state_cache[(slice(None, None, None), slice(None, None, None), slice(0, 1, None))];  l_fn_self_streaming_state_cache = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:120 in step, code: input_ = input_.permute(0, 2, 1).contiguous()
        permute: "i64[1, 1, 17][357, 1, 21]cuda:0" = input_.permute(0, 2, 1);  input_ = None
        input__1: "i64[1, 1, 17][17, 17, 1]cuda:0" = permute.contiguous();  permute = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:122 in step, code: 'input_ids': input_[:, :, 0],
        getitem_25: "i64[1, 1][17, 17]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), 0)]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:123 in step, code: 'speak_ids': input_[:, :, 1:9],
        getitem_26: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(1, 9, None))]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:124 in step, code: 'listen_ids': input_[:, :, 9:],
        getitem_27: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(9, None, None))];  input__1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_25, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq = getitem_25 == None;  getitem_25 = eq = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_28: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_28, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_28 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_29: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_29, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_29 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_30: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_30 += add;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_30;  getitem_30 = add = None
        embeddings[-1] = iadd;  setitem_24 = embeddings;  iadd = setitem_24 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_31: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_31, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_31 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_32: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_32, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_32 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_33: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_1: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_33 += add_1;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_33;  getitem_33 = add_1 = None
        embeddings[-1] = iadd_1;  setitem_25 = embeddings;  iadd_1 = setitem_25 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_34: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_34, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_34 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_35: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_35, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_35 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_36: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_2: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_36 += add_2;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_36;  getitem_36 = add_2 = None
        embeddings[-1] = iadd_2;  setitem_26 = embeddings;  iadd_2 = setitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_37: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_37, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_37 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_38: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_38, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_38 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_39: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_3: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_39 += add_3;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_39;  getitem_39 = add_3 = None
        embeddings[-1] = iadd_3;  setitem_27 = embeddings;  iadd_3 = setitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_40: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_40, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_40 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_41: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_41, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_41 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_42: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_4: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_42 += add_4;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_42;  getitem_42 = add_4 = None
        embeddings[-1] = iadd_4;  setitem_28 = embeddings;  iadd_4 = setitem_28 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_43: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_43, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_43 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_44: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_44, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_44 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_45: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_5: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_45 += add_5;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_45;  getitem_45 = add_5 = None
        embeddings[-1] = iadd_5;  setitem_29 = embeddings;  iadd_5 = setitem_29 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_46: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_46, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_46 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_47: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_47, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_47 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_48: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_6: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_48 += add_6;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_48;  getitem_48 = add_6 = None
        embeddings[-1] = iadd_6;  setitem_30 = embeddings;  iadd_6 = setitem_30 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_49: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 7)];  getitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_49, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_49 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_50: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 7)];  getitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_50, l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_50 = l_fn_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_51: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_7: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_51 += add_7;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_51;  getitem_51 = add_7 = None
        embeddings[-1] = iadd_7;  setitem_31 = embeddings;  iadd_7 = setitem_31 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(0, 1, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_52: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_1: "b8[][]cuda:0" = getitem_52 == 0;  getitem_52 = eq_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_tokens_: "i64[1, 8, 1][8, 1, 1]cuda:0", L_self_streaming_state_cache: "i64[1, 17, 21][357, 21, 1]cuda:0", L_self_streaming_state_initial: "i64[1, 17, 1][17, 1, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0"):
        l_input_tokens_ = L_input_tokens_
        l_self_streaming_state_cache = L_self_streaming_state_cache
        l_self_streaming_state_initial = L_self_streaming_state_initial
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 9, slice(0, 1, None))] = getitem;  setitem = l_self_streaming_state_cache;  getitem = setitem = None
        getitem_1: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 1)]
        l_self_streaming_state_cache[(slice(None, None, None), 10, slice(1, 2, None))] = getitem_1;  setitem_1 = l_self_streaming_state_cache;  getitem_1 = setitem_1 = None
        getitem_2: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 2)]
        l_self_streaming_state_cache[(slice(None, None, None), 11, slice(1, 2, None))] = getitem_2;  setitem_2 = l_self_streaming_state_cache;  getitem_2 = setitem_2 = None
        getitem_3: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 3)]
        l_self_streaming_state_cache[(slice(None, None, None), 12, slice(1, 2, None))] = getitem_3;  setitem_3 = l_self_streaming_state_cache;  getitem_3 = setitem_3 = None
        getitem_4: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 4)]
        l_self_streaming_state_cache[(slice(None, None, None), 13, slice(1, 2, None))] = getitem_4;  setitem_4 = l_self_streaming_state_cache;  getitem_4 = setitem_4 = None
        getitem_5: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 5)]
        l_self_streaming_state_cache[(slice(None, None, None), 14, slice(1, 2, None))] = getitem_5;  setitem_5 = l_self_streaming_state_cache;  getitem_5 = setitem_5 = None
        getitem_6: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 6)]
        l_self_streaming_state_cache[(slice(None, None, None), 15, slice(1, 2, None))] = getitem_6;  setitem_6 = l_self_streaming_state_cache;  getitem_6 = setitem_6 = None
        getitem_7: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 7)];  l_input_tokens_ = None
        l_self_streaming_state_cache[(slice(None, None, None), 16, slice(1, 2, None))] = getitem_7;  setitem_7 = l_self_streaming_state_cache;  getitem_7 = setitem_7 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:118 in step, code: state.cache[:, k, position] = state.initial[:, k, 0]
        getitem_8: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 0, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 0, 0)] = getitem_8;  setitem_8 = l_self_streaming_state_cache;  getitem_8 = setitem_8 = None
        getitem_9: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 1, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 1, 0)] = getitem_9;  setitem_9 = l_self_streaming_state_cache;  getitem_9 = setitem_9 = None
        getitem_10: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 2, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 2, 0)] = getitem_10;  setitem_10 = l_self_streaming_state_cache;  getitem_10 = setitem_10 = None
        getitem_11: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 3, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 3, 0)] = getitem_11;  setitem_11 = l_self_streaming_state_cache;  getitem_11 = setitem_11 = None
        getitem_12: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 4, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 4, 0)] = getitem_12;  setitem_12 = l_self_streaming_state_cache;  getitem_12 = setitem_12 = None
        getitem_13: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 5, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 5, 0)] = getitem_13;  setitem_13 = l_self_streaming_state_cache;  getitem_13 = setitem_13 = None
        getitem_14: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 6, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 6, 0)] = getitem_14;  setitem_14 = l_self_streaming_state_cache;  getitem_14 = setitem_14 = None
        getitem_15: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 7, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 7, 0)] = getitem_15;  setitem_15 = l_self_streaming_state_cache;  getitem_15 = setitem_15 = None
        getitem_16: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 8, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 8, 0)] = getitem_16;  setitem_16 = l_self_streaming_state_cache;  getitem_16 = setitem_16 = None
        getitem_17: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 10, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 10, 0)] = getitem_17;  setitem_17 = l_self_streaming_state_cache;  getitem_17 = setitem_17 = None
        getitem_18: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 11, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 11, 0)] = getitem_18;  setitem_18 = l_self_streaming_state_cache;  getitem_18 = setitem_18 = None
        getitem_19: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 12, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 12, 0)] = getitem_19;  setitem_19 = l_self_streaming_state_cache;  getitem_19 = setitem_19 = None
        getitem_20: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 13, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 13, 0)] = getitem_20;  setitem_20 = l_self_streaming_state_cache;  getitem_20 = setitem_20 = None
        getitem_21: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 14, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 14, 0)] = getitem_21;  setitem_21 = l_self_streaming_state_cache;  getitem_21 = setitem_21 = None
        getitem_22: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 15, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 15, 0)] = getitem_22;  setitem_22 = l_self_streaming_state_cache;  getitem_22 = setitem_22 = None
        getitem_23: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 16, 0)];  l_self_streaming_state_initial = None
        l_self_streaming_state_cache[(slice(None, None, None), 16, 0)] = getitem_23;  setitem_23 = l_self_streaming_state_cache;  getitem_23 = setitem_23 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:119 in step, code: input_ = state.cache[:, :, position : position + 1]
        input_: "i64[1, 17, 1][357, 21, 1]cuda:0" = l_self_streaming_state_cache[(slice(None, None, None), slice(None, None, None), slice(0, 1, None))];  l_self_streaming_state_cache = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:120 in step, code: input_ = input_.permute(0, 2, 1).contiguous()
        permute: "i64[1, 1, 17][357, 1, 21]cuda:0" = input_.permute(0, 2, 1);  input_ = None
        input__1: "i64[1, 1, 17][17, 17, 1]cuda:0" = permute.contiguous();  permute = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:122 in step, code: 'input_ids': input_[:, :, 0],
        getitem_25: "i64[1, 1][17, 17]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), 0)]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:123 in step, code: 'speak_ids': input_[:, :, 1:9],
        getitem_26: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(1, 9, None))]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:124 in step, code: 'listen_ids': input_[:, :, 9:],
        getitem_27: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(9, None, None))];  input__1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_25, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq = getitem_25 == None;  getitem_25 = eq = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_28: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_28, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_28 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_29: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_29, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_29 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_30: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_30 += add;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_30;  getitem_30 = add = None
        embeddings[-1] = iadd;  setitem_24 = embeddings;  iadd = setitem_24 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_31: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_31, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_31 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_32: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_32, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_32 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_33: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_1: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_33 += add_1;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_33;  getitem_33 = add_1 = None
        embeddings[-1] = iadd_1;  setitem_25 = embeddings;  iadd_1 = setitem_25 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_34: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_34, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_34 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_35: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_35, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_35 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_36: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_2: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_36 += add_2;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_36;  getitem_36 = add_2 = None
        embeddings[-1] = iadd_2;  setitem_26 = embeddings;  iadd_2 = setitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_37: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_37, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_37 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_38: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_38, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_38 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_39: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_3: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_39 += add_3;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_39;  getitem_39 = add_3 = None
        embeddings[-1] = iadd_3;  setitem_27 = embeddings;  iadd_3 = setitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_40: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_40, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_40 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_41: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_41, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_41 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_42: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_4: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_42 += add_4;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_42;  getitem_42 = add_4 = None
        embeddings[-1] = iadd_4;  setitem_28 = embeddings;  iadd_4 = setitem_28 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_43: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_43, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_43 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_44: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_44, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_44 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_45: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_5: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_45 += add_5;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_45;  getitem_45 = add_5 = None
        embeddings[-1] = iadd_5;  setitem_29 = embeddings;  iadd_5 = setitem_29 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_46: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_46, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_46 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_47: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_47, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_47 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_48: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_6: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_48 += add_6;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_48;  getitem_48 = add_6 = None
        embeddings[-1] = iadd_6;  setitem_30 = embeddings;  iadd_6 = setitem_30 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_49: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 7)];  getitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_49, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_49 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_50: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 7)];  getitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_50, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_50 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_51: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_7: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_51 += add_7;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_51;  getitem_51 = add_7 = None
        embeddings[-1] = iadd_7;  setitem_31 = embeddings;  iadd_7 = setitem_31 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(0, 1, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_52: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_1: "b8[][]cuda:0" = getitem_52 == 0;  getitem_52 = eq_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_tokens_: "i64[1, 8, 1][8, 1, 1]cuda:0", L_self_streaming_state_cache: "i64[1, 17, 21][357, 21, 1]cuda:0", L_self_streaming_state_initial: "i64[1, 17, 1][17, 1, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0"):
        l_input_tokens_ = L_input_tokens_
        l_self_streaming_state_cache = L_self_streaming_state_cache
        l_self_streaming_state_initial = L_self_streaming_state_initial
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 9, slice(0, 1, None))] = getitem;  setitem = l_self_streaming_state_cache;  getitem = setitem = None
        getitem_1: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 1)]
        l_self_streaming_state_cache[(slice(None, None, None), 10, slice(1, 2, None))] = getitem_1;  setitem_1 = l_self_streaming_state_cache;  getitem_1 = setitem_1 = None
        getitem_2: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 2)]
        l_self_streaming_state_cache[(slice(None, None, None), 11, slice(1, 2, None))] = getitem_2;  setitem_2 = l_self_streaming_state_cache;  getitem_2 = setitem_2 = None
        getitem_3: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 3)]
        l_self_streaming_state_cache[(slice(None, None, None), 12, slice(1, 2, None))] = getitem_3;  setitem_3 = l_self_streaming_state_cache;  getitem_3 = setitem_3 = None
        getitem_4: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 4)]
        l_self_streaming_state_cache[(slice(None, None, None), 13, slice(1, 2, None))] = getitem_4;  setitem_4 = l_self_streaming_state_cache;  getitem_4 = setitem_4 = None
        getitem_5: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 5)]
        l_self_streaming_state_cache[(slice(None, None, None), 14, slice(1, 2, None))] = getitem_5;  setitem_5 = l_self_streaming_state_cache;  getitem_5 = setitem_5 = None
        getitem_6: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 6)]
        l_self_streaming_state_cache[(slice(None, None, None), 15, slice(1, 2, None))] = getitem_6;  setitem_6 = l_self_streaming_state_cache;  getitem_6 = setitem_6 = None
        getitem_7: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 7)];  l_input_tokens_ = None
        l_self_streaming_state_cache[(slice(None, None, None), 16, slice(1, 2, None))] = getitem_7;  setitem_7 = l_self_streaming_state_cache;  getitem_7 = setitem_7 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:118 in step, code: state.cache[:, k, position] = state.initial[:, k, 0]
        getitem_8: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 0, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 0, 0)] = getitem_8;  setitem_8 = l_self_streaming_state_cache;  getitem_8 = setitem_8 = None
        getitem_9: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 1, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 1, 0)] = getitem_9;  setitem_9 = l_self_streaming_state_cache;  getitem_9 = setitem_9 = None
        getitem_10: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 2, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 2, 0)] = getitem_10;  setitem_10 = l_self_streaming_state_cache;  getitem_10 = setitem_10 = None
        getitem_11: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 3, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 3, 0)] = getitem_11;  setitem_11 = l_self_streaming_state_cache;  getitem_11 = setitem_11 = None
        getitem_12: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 4, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 4, 0)] = getitem_12;  setitem_12 = l_self_streaming_state_cache;  getitem_12 = setitem_12 = None
        getitem_13: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 5, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 5, 0)] = getitem_13;  setitem_13 = l_self_streaming_state_cache;  getitem_13 = setitem_13 = None
        getitem_14: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 6, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 6, 0)] = getitem_14;  setitem_14 = l_self_streaming_state_cache;  getitem_14 = setitem_14 = None
        getitem_15: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 7, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 7, 0)] = getitem_15;  setitem_15 = l_self_streaming_state_cache;  getitem_15 = setitem_15 = None
        getitem_16: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 8, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 8, 0)] = getitem_16;  setitem_16 = l_self_streaming_state_cache;  getitem_16 = setitem_16 = None
        getitem_17: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 10, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 10, 0)] = getitem_17;  setitem_17 = l_self_streaming_state_cache;  getitem_17 = setitem_17 = None
        getitem_18: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 11, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 11, 0)] = getitem_18;  setitem_18 = l_self_streaming_state_cache;  getitem_18 = setitem_18 = None
        getitem_19: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 12, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 12, 0)] = getitem_19;  setitem_19 = l_self_streaming_state_cache;  getitem_19 = setitem_19 = None
        getitem_20: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 13, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 13, 0)] = getitem_20;  setitem_20 = l_self_streaming_state_cache;  getitem_20 = setitem_20 = None
        getitem_21: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 14, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 14, 0)] = getitem_21;  setitem_21 = l_self_streaming_state_cache;  getitem_21 = setitem_21 = None
        getitem_22: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 15, 0)]
        l_self_streaming_state_cache[(slice(None, None, None), 15, 0)] = getitem_22;  setitem_22 = l_self_streaming_state_cache;  getitem_22 = setitem_22 = None
        getitem_23: "i64[1][17]cuda:0" = l_self_streaming_state_initial[(slice(None, None, None), 16, 0)];  l_self_streaming_state_initial = None
        l_self_streaming_state_cache[(slice(None, None, None), 16, 0)] = getitem_23;  setitem_23 = l_self_streaming_state_cache;  getitem_23 = setitem_23 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:119 in step, code: input_ = state.cache[:, :, position : position + 1]
        input_: "i64[1, 17, 1][357, 21, 1]cuda:0" = l_self_streaming_state_cache[(slice(None, None, None), slice(None, None, None), slice(0, 1, None))];  l_self_streaming_state_cache = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:120 in step, code: input_ = input_.permute(0, 2, 1).contiguous()
        permute: "i64[1, 1, 17][357, 1, 21]cuda:0" = input_.permute(0, 2, 1);  input_ = None
        input__1: "i64[1, 1, 17][17, 17, 1]cuda:0" = permute.contiguous();  permute = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:122 in step, code: 'input_ids': input_[:, :, 0],
        getitem_25: "i64[1, 1][17, 17]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), 0)]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:123 in step, code: 'speak_ids': input_[:, :, 1:9],
        getitem_26: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(1, 9, None))]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:124 in step, code: 'listen_ids': input_[:, :, 9:],
        getitem_27: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(9, None, None))];  input__1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_25, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq = getitem_25 == None;  getitem_25 = eq = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_28: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_28, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_28 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_29: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_29, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_29 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_30: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_30 += add;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_30;  getitem_30 = add = None
        embeddings[-1] = iadd;  setitem_24 = embeddings;  iadd = setitem_24 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_31: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_31, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_31 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_32: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_32, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_32 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_33: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_1: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_33 += add_1;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_33;  getitem_33 = add_1 = None
        embeddings[-1] = iadd_1;  setitem_25 = embeddings;  iadd_1 = setitem_25 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_34: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_34, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_34 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_35: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_35, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_35 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_36: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_2: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_36 += add_2;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_36;  getitem_36 = add_2 = None
        embeddings[-1] = iadd_2;  setitem_26 = embeddings;  iadd_2 = setitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_37: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_37, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_37 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_38: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_38, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_38 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_39: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_3: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_39 += add_3;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_39;  getitem_39 = add_3 = None
        embeddings[-1] = iadd_3;  setitem_27 = embeddings;  iadd_3 = setitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_40: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_40, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_40 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_41: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_41, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_41 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_42: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_4: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_42 += add_4;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_42;  getitem_42 = add_4 = None
        embeddings[-1] = iadd_4;  setitem_28 = embeddings;  iadd_4 = setitem_28 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_43: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_43, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_43 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_44: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_44, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_44 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_45: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_5: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_45 += add_5;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_45;  getitem_45 = add_5 = None
        embeddings[-1] = iadd_5;  setitem_29 = embeddings;  iadd_5 = setitem_29 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_46: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_46, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_46 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_47: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_47, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_47 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_48: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_6: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_48 += add_6;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_48;  getitem_48 = add_6 = None
        embeddings[-1] = iadd_6;  setitem_30 = embeddings;  iadd_6 = setitem_30 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_49: "i64[1, 1][17, 17]cuda:0" = getitem_26[(Ellipsis, 7)];  getitem_26 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_49, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_49 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_50: "i64[1, 1][17, 17]cuda:0" = getitem_27[(Ellipsis, 7)];  getitem_27 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_50, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_50 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_51: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_7: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_51 += add_7;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_51;  getitem_51 = add_7 = None
        embeddings[-1] = iadd_7;  setitem_31 = embeddings;  iadd_7 = setitem_31 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(0, 1, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_52: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_1: "b8[][]cuda:0" = getitem_52 == 0;  getitem_52 = eq_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_listen_ids_: "i64[1, 1, 8][17, 17, 1]cuda:0", L_input_ids_: "i64[1, 1][17, 17]cuda:0", L_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_speak_ids_: "i64[1, 1, 8][17, 17, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0"):
        l_listen_ids_ = L_listen_ids_
        l_input_ids_ = L_input_ids_
        l_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_speak_ids_ = L_speak_ids_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq = l_input_ids_ == None;  l_input_ids_ = eq = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_1: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_1, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_1 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_2: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_2 += add;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_2;  getitem_2 = add = None
        embeddings[-1] = iadd;  setitem = embeddings;  iadd = setitem = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_3: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_3, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_3 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_4: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_4, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_4 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_5: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_1: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_5 += add_1;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_5;  getitem_5 = add_1 = None
        embeddings[-1] = iadd_1;  setitem_1 = embeddings;  iadd_1 = setitem_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_6: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_6, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_6 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_7: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_7, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_7 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_8: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_2: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_8 += add_2;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_8;  getitem_8 = add_2 = None
        embeddings[-1] = iadd_2;  setitem_2 = embeddings;  iadd_2 = setitem_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_9: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_9, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_9 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_10: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_10, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_10 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_11: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_3: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_11 += add_3;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_11;  getitem_11 = add_3 = None
        embeddings[-1] = iadd_3;  setitem_3 = embeddings;  iadd_3 = setitem_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_12: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_12, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_12 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_13: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_13, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_13 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_14: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_4: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_14 += add_4;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_14;  getitem_14 = add_4 = None
        embeddings[-1] = iadd_4;  setitem_4 = embeddings;  iadd_4 = setitem_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_15: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_15, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_15 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_16: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_16, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_16 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_17: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_5: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_17 += add_5;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_17;  getitem_17 = add_5 = None
        embeddings[-1] = iadd_5;  setitem_5 = embeddings;  iadd_5 = setitem_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_18: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_18, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_18 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_19: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_19, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_19 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_20: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_6: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_20 += add_6;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_20;  getitem_20 = add_6 = None
        embeddings[-1] = iadd_6;  setitem_6 = embeddings;  iadd_6 = setitem_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_21: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 7)];  l_speak_ids_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_21, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_21 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_22: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 7)];  l_listen_ids_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_22, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_22 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_23: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_7: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_23 += add_7;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_23;  getitem_23 = add_7 = None
        embeddings[-1] = iadd_7;  setitem_7 = embeddings;  iadd_7 = setitem_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(0, 1, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_24: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_1: "b8[][]cuda:0" = getitem_24 == 0;  getitem_24 = eq_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_tokens_: "i64[1, 8, 1][8, 1, 1]cuda:0", L_self_streaming_state_cache: "i64[1, 17, 21][357, 21, 1]cuda:0", L_self_streaming_state_offset: "Sym(s0)", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_streaming_state_past_key_values_0_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_0_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_1_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_1_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_2_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_2_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_3_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_3_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_4_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_4_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_5_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_5_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_6_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_6_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_7_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_7_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_8_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_8_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_9_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_9_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_10_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_10_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_11_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_11_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_12_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_12_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_13_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_13_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_14_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_14_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_15_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_15_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_16_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_16_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_17_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_17_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_18_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_18_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_19_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_19_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_20_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_20_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_21_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_21_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_22_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_22_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_23_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_23_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_24_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_24_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_25_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_25_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_26_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_26_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_27_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_27_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0"):
        l_input_tokens_ = L_input_tokens_
        l_self_streaming_state_cache = L_self_streaming_state_cache
        l_self_streaming_state_offset = L_self_streaming_state_offset
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        l_self_streaming_state_past_key_values_0_0_ = L_self_streaming_state_past_key_values_0_0_
        l_self_streaming_state_past_key_values_0_1_ = L_self_streaming_state_past_key_values_0_1_
        l_self_streaming_state_past_key_values_1_0_ = L_self_streaming_state_past_key_values_1_0_
        l_self_streaming_state_past_key_values_1_1_ = L_self_streaming_state_past_key_values_1_1_
        l_self_streaming_state_past_key_values_2_0_ = L_self_streaming_state_past_key_values_2_0_
        l_self_streaming_state_past_key_values_2_1_ = L_self_streaming_state_past_key_values_2_1_
        l_self_streaming_state_past_key_values_3_0_ = L_self_streaming_state_past_key_values_3_0_
        l_self_streaming_state_past_key_values_3_1_ = L_self_streaming_state_past_key_values_3_1_
        l_self_streaming_state_past_key_values_4_0_ = L_self_streaming_state_past_key_values_4_0_
        l_self_streaming_state_past_key_values_4_1_ = L_self_streaming_state_past_key_values_4_1_
        l_self_streaming_state_past_key_values_5_0_ = L_self_streaming_state_past_key_values_5_0_
        l_self_streaming_state_past_key_values_5_1_ = L_self_streaming_state_past_key_values_5_1_
        l_self_streaming_state_past_key_values_6_0_ = L_self_streaming_state_past_key_values_6_0_
        l_self_streaming_state_past_key_values_6_1_ = L_self_streaming_state_past_key_values_6_1_
        l_self_streaming_state_past_key_values_7_0_ = L_self_streaming_state_past_key_values_7_0_
        l_self_streaming_state_past_key_values_7_1_ = L_self_streaming_state_past_key_values_7_1_
        l_self_streaming_state_past_key_values_8_0_ = L_self_streaming_state_past_key_values_8_0_
        l_self_streaming_state_past_key_values_8_1_ = L_self_streaming_state_past_key_values_8_1_
        l_self_streaming_state_past_key_values_9_0_ = L_self_streaming_state_past_key_values_9_0_
        l_self_streaming_state_past_key_values_9_1_ = L_self_streaming_state_past_key_values_9_1_
        l_self_streaming_state_past_key_values_10_0_ = L_self_streaming_state_past_key_values_10_0_
        l_self_streaming_state_past_key_values_10_1_ = L_self_streaming_state_past_key_values_10_1_
        l_self_streaming_state_past_key_values_11_0_ = L_self_streaming_state_past_key_values_11_0_
        l_self_streaming_state_past_key_values_11_1_ = L_self_streaming_state_past_key_values_11_1_
        l_self_streaming_state_past_key_values_12_0_ = L_self_streaming_state_past_key_values_12_0_
        l_self_streaming_state_past_key_values_12_1_ = L_self_streaming_state_past_key_values_12_1_
        l_self_streaming_state_past_key_values_13_0_ = L_self_streaming_state_past_key_values_13_0_
        l_self_streaming_state_past_key_values_13_1_ = L_self_streaming_state_past_key_values_13_1_
        l_self_streaming_state_past_key_values_14_0_ = L_self_streaming_state_past_key_values_14_0_
        l_self_streaming_state_past_key_values_14_1_ = L_self_streaming_state_past_key_values_14_1_
        l_self_streaming_state_past_key_values_15_0_ = L_self_streaming_state_past_key_values_15_0_
        l_self_streaming_state_past_key_values_15_1_ = L_self_streaming_state_past_key_values_15_1_
        l_self_streaming_state_past_key_values_16_0_ = L_self_streaming_state_past_key_values_16_0_
        l_self_streaming_state_past_key_values_16_1_ = L_self_streaming_state_past_key_values_16_1_
        l_self_streaming_state_past_key_values_17_0_ = L_self_streaming_state_past_key_values_17_0_
        l_self_streaming_state_past_key_values_17_1_ = L_self_streaming_state_past_key_values_17_1_
        l_self_streaming_state_past_key_values_18_0_ = L_self_streaming_state_past_key_values_18_0_
        l_self_streaming_state_past_key_values_18_1_ = L_self_streaming_state_past_key_values_18_1_
        l_self_streaming_state_past_key_values_19_0_ = L_self_streaming_state_past_key_values_19_0_
        l_self_streaming_state_past_key_values_19_1_ = L_self_streaming_state_past_key_values_19_1_
        l_self_streaming_state_past_key_values_20_0_ = L_self_streaming_state_past_key_values_20_0_
        l_self_streaming_state_past_key_values_20_1_ = L_self_streaming_state_past_key_values_20_1_
        l_self_streaming_state_past_key_values_21_0_ = L_self_streaming_state_past_key_values_21_0_
        l_self_streaming_state_past_key_values_21_1_ = L_self_streaming_state_past_key_values_21_1_
        l_self_streaming_state_past_key_values_22_0_ = L_self_streaming_state_past_key_values_22_0_
        l_self_streaming_state_past_key_values_22_1_ = L_self_streaming_state_past_key_values_22_1_
        l_self_streaming_state_past_key_values_23_0_ = L_self_streaming_state_past_key_values_23_0_
        l_self_streaming_state_past_key_values_23_1_ = L_self_streaming_state_past_key_values_23_1_
        l_self_streaming_state_past_key_values_24_0_ = L_self_streaming_state_past_key_values_24_0_
        l_self_streaming_state_past_key_values_24_1_ = L_self_streaming_state_past_key_values_24_1_
        l_self_streaming_state_past_key_values_25_0_ = L_self_streaming_state_past_key_values_25_0_
        l_self_streaming_state_past_key_values_25_1_ = L_self_streaming_state_past_key_values_25_1_
        l_self_streaming_state_past_key_values_26_0_ = L_self_streaming_state_past_key_values_26_0_
        l_self_streaming_state_past_key_values_26_1_ = L_self_streaming_state_past_key_values_26_1_
        l_self_streaming_state_past_key_values_27_0_ = L_self_streaming_state_past_key_values_27_0_
        l_self_streaming_state_past_key_values_27_1_ = L_self_streaming_state_past_key_values_27_1_
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add: "Sym(s0)" = l_self_streaming_state_offset + 0
        mod: "Sym(PythonMod(s0, 21))" = add % 21;  add = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 0)]
        add_1: "Sym((PythonMod(s0, 21)) + 1)" = mod + 1
        l_self_streaming_state_cache[(slice(None, None, None), 9, slice(mod, add_1, None))] = getitem;  setitem = l_self_streaming_state_cache;  mod = add_1 = getitem = setitem = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_2: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_1: "Sym(PythonMod(s0 + 1, 21))" = add_2 % 21;  add_2 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_1: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 1)]
        add_3: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_1 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 10, slice(mod_1, add_3, None))] = getitem_1;  setitem_1 = l_self_streaming_state_cache;  mod_1 = add_3 = getitem_1 = setitem_1 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_4: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_2: "Sym(PythonMod(s0 + 1, 21))" = add_4 % 21;  add_4 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_2: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 2)]
        add_5: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_2 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 11, slice(mod_2, add_5, None))] = getitem_2;  setitem_2 = l_self_streaming_state_cache;  mod_2 = add_5 = getitem_2 = setitem_2 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_6: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_3: "Sym(PythonMod(s0 + 1, 21))" = add_6 % 21;  add_6 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_3: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 3)]
        add_7: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_3 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 12, slice(mod_3, add_7, None))] = getitem_3;  setitem_3 = l_self_streaming_state_cache;  mod_3 = add_7 = getitem_3 = setitem_3 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_8: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_4: "Sym(PythonMod(s0 + 1, 21))" = add_8 % 21;  add_8 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_4: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 4)]
        add_9: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_4 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 13, slice(mod_4, add_9, None))] = getitem_4;  setitem_4 = l_self_streaming_state_cache;  mod_4 = add_9 = getitem_4 = setitem_4 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_10: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_5: "Sym(PythonMod(s0 + 1, 21))" = add_10 % 21;  add_10 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_5: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 5)]
        add_11: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_5 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 14, slice(mod_5, add_11, None))] = getitem_5;  setitem_5 = l_self_streaming_state_cache;  mod_5 = add_11 = getitem_5 = setitem_5 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_12: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_6: "Sym(PythonMod(s0 + 1, 21))" = add_12 % 21;  add_12 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_6: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 6)]
        add_13: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_6 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 15, slice(mod_6, add_13, None))] = getitem_6;  setitem_6 = l_self_streaming_state_cache;  mod_6 = add_13 = getitem_6 = setitem_6 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_14: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_7: "Sym(PythonMod(s0 + 1, 21))" = add_14 % 21;  add_14 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_7: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 7)];  l_input_tokens_ = None
        add_15: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_7 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 16, slice(mod_7, add_15, None))] = getitem_7;  setitem_7 = l_self_streaming_state_cache;  mod_7 = add_15 = getitem_7 = setitem_7 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:115 in step, code: position = state.offset % CT
        mod_8: "Sym(PythonMod(s0, 21))" = l_self_streaming_state_offset % 21
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:117 in step, code: if state.offset == 0 and k <= 8 or state.offset < delay and k > 8:
        eq: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq = None
        lt: "Sym(s0 < 0)" = l_self_streaming_state_offset < 0;  lt = None
        eq_1: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_1 = None
        lt_1: "Sym(s0 < 0)" = l_self_streaming_state_offset < 0;  lt_1 = None
        eq_2: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_2 = None
        lt_2: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_2 = None
        eq_3: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_3 = None
        lt_3: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_3 = None
        eq_4: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_4 = None
        lt_4: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_4 = None
        eq_5: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_5 = None
        lt_5: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_5 = None
        eq_6: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_6 = None
        lt_6: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_6 = None
        eq_7: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_7 = None
        lt_7: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_7 = None
        eq_8: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_8 = None
        lt_8: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_8 = None
        eq_9: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_9 = None
        lt_9: "Sym(s0 < 0)" = l_self_streaming_state_offset < 0;  lt_9 = None
        eq_10: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_10 = None
        lt_10: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_10 = None
        eq_11: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_11 = None
        lt_11: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_11 = None
        eq_12: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_12 = None
        lt_12: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_12 = None
        eq_13: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_13 = None
        lt_13: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_13 = None
        eq_14: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_14 = None
        lt_14: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_14 = None
        eq_15: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_15 = None
        lt_15: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_15 = None
        eq_16: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_16 = None
        lt_16: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  l_self_streaming_state_offset = lt_16 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:119 in step, code: input_ = state.cache[:, :, position : position + 1]
        add_16: "Sym((PythonMod(s0, 21)) + 1)" = mod_8 + 1
        input_: "i64[1, 17, 1][357, 21, 1]cuda:0" = l_self_streaming_state_cache[(slice(None, None, None), slice(None, None, None), slice(mod_8, add_16, None))];  l_self_streaming_state_cache = mod_8 = add_16 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:120 in step, code: input_ = input_.permute(0, 2, 1).contiguous()
        permute: "i64[1, 1, 17][357, 1, 21]cuda:0" = input_.permute(0, 2, 1);  input_ = None
        input__1: "i64[1, 1, 17][17, 17, 1]cuda:0" = permute.contiguous();  permute = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:122 in step, code: 'input_ids': input_[:, :, 0],
        getitem_9: "i64[1, 1][17, 17]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), 0)]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:123 in step, code: 'speak_ids': input_[:, :, 1:9],
        getitem_10: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(1, 9, None))]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:124 in step, code: 'listen_ids': input_[:, :, 9:],
        getitem_11: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(9, None, None))];  input__1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_9, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq_17 = getitem_9 == None;  getitem_9 = eq_17 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_12: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_12, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_12 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_13: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_13, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_13 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_14: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_17: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_14 += add_17;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_14;  getitem_14 = add_17 = None
        embeddings[-1] = iadd;  setitem_8 = embeddings;  iadd = setitem_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_15: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_15, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_15 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_16: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_16, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_16 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_17: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_18: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_17 += add_18;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_17;  getitem_17 = add_18 = None
        embeddings[-1] = iadd_1;  setitem_9 = embeddings;  iadd_1 = setitem_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_18: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_18, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_18 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_19: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_19, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_19 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_20: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_19: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_20 += add_19;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_20;  getitem_20 = add_19 = None
        embeddings[-1] = iadd_2;  setitem_10 = embeddings;  iadd_2 = setitem_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_21: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_21, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_21 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_22: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_22, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_22 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_23: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_20: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_23 += add_20;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_23;  getitem_23 = add_20 = None
        embeddings[-1] = iadd_3;  setitem_11 = embeddings;  iadd_3 = setitem_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_24: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_24, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_24 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_25: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_25, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_25 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_26: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_21: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_26 += add_21;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_26;  getitem_26 = add_21 = None
        embeddings[-1] = iadd_4;  setitem_12 = embeddings;  iadd_4 = setitem_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_27: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_27, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_27 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_28: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_28, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_28 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_29: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_22: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_29 += add_22;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_29;  getitem_29 = add_22 = None
        embeddings[-1] = iadd_5;  setitem_13 = embeddings;  iadd_5 = setitem_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_30: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_30, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_30 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_31: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_31, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_31 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_32: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_23: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_32 += add_23;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_32;  getitem_32 = add_23 = None
        embeddings[-1] = iadd_6;  setitem_14 = embeddings;  iadd_6 = setitem_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_33: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 7)];  getitem_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_33, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_33 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_34: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 7)];  getitem_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_34, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_34 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_35: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_24: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_35 += add_24;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_35;  getitem_35 = add_24 = None
        embeddings[-1] = iadd_7;  setitem_15 = embeddings;  iadd_7 = setitem_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(1, 2, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_36: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_18: "b8[][]cuda:0" = getitem_36 == 0;  getitem_36 = eq_18 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_tokens_: "i64[1, 8, 1][8, 1, 1]cuda:0", L_self_streaming_state_cache: "i64[1, 17, 21][357, 21, 1]cuda:0", L_self_streaming_state_offset: "Sym(s0)", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_streaming_state_past_key_values_0_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_0_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_1_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_1_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_2_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_2_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_3_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_3_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_4_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_4_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_5_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_5_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_6_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_6_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_7_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_7_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_8_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_8_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_9_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_9_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_10_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_10_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_11_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_11_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_12_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_12_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_13_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_13_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_14_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_14_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_15_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_15_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_16_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_16_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_17_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_17_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_18_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_18_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_19_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_19_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_20_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_20_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_21_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_21_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_22_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_22_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_23_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_23_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_24_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_24_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_25_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_25_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_26_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_26_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_self_streaming_state_past_key_values_27_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_self_streaming_state_past_key_values_27_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0"):
        l_input_tokens_ = L_input_tokens_
        l_self_streaming_state_cache = L_self_streaming_state_cache
        l_self_streaming_state_offset = L_self_streaming_state_offset
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        l_self_streaming_state_past_key_values_0_0_ = L_self_streaming_state_past_key_values_0_0_
        l_self_streaming_state_past_key_values_0_1_ = L_self_streaming_state_past_key_values_0_1_
        l_self_streaming_state_past_key_values_1_0_ = L_self_streaming_state_past_key_values_1_0_
        l_self_streaming_state_past_key_values_1_1_ = L_self_streaming_state_past_key_values_1_1_
        l_self_streaming_state_past_key_values_2_0_ = L_self_streaming_state_past_key_values_2_0_
        l_self_streaming_state_past_key_values_2_1_ = L_self_streaming_state_past_key_values_2_1_
        l_self_streaming_state_past_key_values_3_0_ = L_self_streaming_state_past_key_values_3_0_
        l_self_streaming_state_past_key_values_3_1_ = L_self_streaming_state_past_key_values_3_1_
        l_self_streaming_state_past_key_values_4_0_ = L_self_streaming_state_past_key_values_4_0_
        l_self_streaming_state_past_key_values_4_1_ = L_self_streaming_state_past_key_values_4_1_
        l_self_streaming_state_past_key_values_5_0_ = L_self_streaming_state_past_key_values_5_0_
        l_self_streaming_state_past_key_values_5_1_ = L_self_streaming_state_past_key_values_5_1_
        l_self_streaming_state_past_key_values_6_0_ = L_self_streaming_state_past_key_values_6_0_
        l_self_streaming_state_past_key_values_6_1_ = L_self_streaming_state_past_key_values_6_1_
        l_self_streaming_state_past_key_values_7_0_ = L_self_streaming_state_past_key_values_7_0_
        l_self_streaming_state_past_key_values_7_1_ = L_self_streaming_state_past_key_values_7_1_
        l_self_streaming_state_past_key_values_8_0_ = L_self_streaming_state_past_key_values_8_0_
        l_self_streaming_state_past_key_values_8_1_ = L_self_streaming_state_past_key_values_8_1_
        l_self_streaming_state_past_key_values_9_0_ = L_self_streaming_state_past_key_values_9_0_
        l_self_streaming_state_past_key_values_9_1_ = L_self_streaming_state_past_key_values_9_1_
        l_self_streaming_state_past_key_values_10_0_ = L_self_streaming_state_past_key_values_10_0_
        l_self_streaming_state_past_key_values_10_1_ = L_self_streaming_state_past_key_values_10_1_
        l_self_streaming_state_past_key_values_11_0_ = L_self_streaming_state_past_key_values_11_0_
        l_self_streaming_state_past_key_values_11_1_ = L_self_streaming_state_past_key_values_11_1_
        l_self_streaming_state_past_key_values_12_0_ = L_self_streaming_state_past_key_values_12_0_
        l_self_streaming_state_past_key_values_12_1_ = L_self_streaming_state_past_key_values_12_1_
        l_self_streaming_state_past_key_values_13_0_ = L_self_streaming_state_past_key_values_13_0_
        l_self_streaming_state_past_key_values_13_1_ = L_self_streaming_state_past_key_values_13_1_
        l_self_streaming_state_past_key_values_14_0_ = L_self_streaming_state_past_key_values_14_0_
        l_self_streaming_state_past_key_values_14_1_ = L_self_streaming_state_past_key_values_14_1_
        l_self_streaming_state_past_key_values_15_0_ = L_self_streaming_state_past_key_values_15_0_
        l_self_streaming_state_past_key_values_15_1_ = L_self_streaming_state_past_key_values_15_1_
        l_self_streaming_state_past_key_values_16_0_ = L_self_streaming_state_past_key_values_16_0_
        l_self_streaming_state_past_key_values_16_1_ = L_self_streaming_state_past_key_values_16_1_
        l_self_streaming_state_past_key_values_17_0_ = L_self_streaming_state_past_key_values_17_0_
        l_self_streaming_state_past_key_values_17_1_ = L_self_streaming_state_past_key_values_17_1_
        l_self_streaming_state_past_key_values_18_0_ = L_self_streaming_state_past_key_values_18_0_
        l_self_streaming_state_past_key_values_18_1_ = L_self_streaming_state_past_key_values_18_1_
        l_self_streaming_state_past_key_values_19_0_ = L_self_streaming_state_past_key_values_19_0_
        l_self_streaming_state_past_key_values_19_1_ = L_self_streaming_state_past_key_values_19_1_
        l_self_streaming_state_past_key_values_20_0_ = L_self_streaming_state_past_key_values_20_0_
        l_self_streaming_state_past_key_values_20_1_ = L_self_streaming_state_past_key_values_20_1_
        l_self_streaming_state_past_key_values_21_0_ = L_self_streaming_state_past_key_values_21_0_
        l_self_streaming_state_past_key_values_21_1_ = L_self_streaming_state_past_key_values_21_1_
        l_self_streaming_state_past_key_values_22_0_ = L_self_streaming_state_past_key_values_22_0_
        l_self_streaming_state_past_key_values_22_1_ = L_self_streaming_state_past_key_values_22_1_
        l_self_streaming_state_past_key_values_23_0_ = L_self_streaming_state_past_key_values_23_0_
        l_self_streaming_state_past_key_values_23_1_ = L_self_streaming_state_past_key_values_23_1_
        l_self_streaming_state_past_key_values_24_0_ = L_self_streaming_state_past_key_values_24_0_
        l_self_streaming_state_past_key_values_24_1_ = L_self_streaming_state_past_key_values_24_1_
        l_self_streaming_state_past_key_values_25_0_ = L_self_streaming_state_past_key_values_25_0_
        l_self_streaming_state_past_key_values_25_1_ = L_self_streaming_state_past_key_values_25_1_
        l_self_streaming_state_past_key_values_26_0_ = L_self_streaming_state_past_key_values_26_0_
        l_self_streaming_state_past_key_values_26_1_ = L_self_streaming_state_past_key_values_26_1_
        l_self_streaming_state_past_key_values_27_0_ = L_self_streaming_state_past_key_values_27_0_
        l_self_streaming_state_past_key_values_27_1_ = L_self_streaming_state_past_key_values_27_1_
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add: "Sym(s0)" = l_self_streaming_state_offset + 0
        mod: "Sym(PythonMod(s0, 21))" = add % 21;  add = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 0)]
        add_1: "Sym((PythonMod(s0, 21)) + 1)" = mod + 1
        l_self_streaming_state_cache[(slice(None, None, None), 9, slice(mod, add_1, None))] = getitem;  setitem = l_self_streaming_state_cache;  mod = add_1 = getitem = setitem = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_2: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_1: "Sym(PythonMod(s0 + 1, 21))" = add_2 % 21;  add_2 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_1: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 1)]
        add_3: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_1 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 10, slice(mod_1, add_3, None))] = getitem_1;  setitem_1 = l_self_streaming_state_cache;  mod_1 = add_3 = getitem_1 = setitem_1 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_4: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_2: "Sym(PythonMod(s0 + 1, 21))" = add_4 % 21;  add_4 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_2: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 2)]
        add_5: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_2 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 11, slice(mod_2, add_5, None))] = getitem_2;  setitem_2 = l_self_streaming_state_cache;  mod_2 = add_5 = getitem_2 = setitem_2 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_6: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_3: "Sym(PythonMod(s0 + 1, 21))" = add_6 % 21;  add_6 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_3: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 3)]
        add_7: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_3 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 12, slice(mod_3, add_7, None))] = getitem_3;  setitem_3 = l_self_streaming_state_cache;  mod_3 = add_7 = getitem_3 = setitem_3 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_8: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_4: "Sym(PythonMod(s0 + 1, 21))" = add_8 % 21;  add_8 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_4: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 4)]
        add_9: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_4 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 13, slice(mod_4, add_9, None))] = getitem_4;  setitem_4 = l_self_streaming_state_cache;  mod_4 = add_9 = getitem_4 = setitem_4 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_10: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_5: "Sym(PythonMod(s0 + 1, 21))" = add_10 % 21;  add_10 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_5: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 5)]
        add_11: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_5 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 14, slice(mod_5, add_11, None))] = getitem_5;  setitem_5 = l_self_streaming_state_cache;  mod_5 = add_11 = getitem_5 = setitem_5 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_12: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_6: "Sym(PythonMod(s0 + 1, 21))" = add_12 % 21;  add_12 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_6: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 6)]
        add_13: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_6 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 15, slice(mod_6, add_13, None))] = getitem_6;  setitem_6 = l_self_streaming_state_cache;  mod_6 = add_13 = getitem_6 = setitem_6 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:112 in step, code: write_position = (state.offset + delay) % CT
        add_14: "Sym(s0 + 1)" = l_self_streaming_state_offset + 1
        mod_7: "Sym(PythonMod(s0 + 1, 21))" = add_14 % 21;  add_14 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:113 in step, code: state.cache[:, k, write_position : write_position + 1] = input_tokens[:, q_other]
        getitem_7: "i64[1, 1][8, 1]cuda:0" = l_input_tokens_[(slice(None, None, None), 7)];  l_input_tokens_ = None
        add_15: "Sym((PythonMod(s0 + 1, 21)) + 1)" = mod_7 + 1
        l_self_streaming_state_cache[(slice(None, None, None), 16, slice(mod_7, add_15, None))] = getitem_7;  setitem_7 = l_self_streaming_state_cache;  mod_7 = add_15 = getitem_7 = setitem_7 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:115 in step, code: position = state.offset % CT
        mod_8: "Sym(PythonMod(s0, 21))" = l_self_streaming_state_offset % 21
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:117 in step, code: if state.offset == 0 and k <= 8 or state.offset < delay and k > 8:
        eq: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq = None
        lt: "Sym(s0 < 0)" = l_self_streaming_state_offset < 0;  lt = None
        eq_1: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_1 = None
        lt_1: "Sym(s0 < 0)" = l_self_streaming_state_offset < 0;  lt_1 = None
        eq_2: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_2 = None
        lt_2: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_2 = None
        eq_3: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_3 = None
        lt_3: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_3 = None
        eq_4: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_4 = None
        lt_4: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_4 = None
        eq_5: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_5 = None
        lt_5: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_5 = None
        eq_6: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_6 = None
        lt_6: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_6 = None
        eq_7: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_7 = None
        lt_7: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_7 = None
        eq_8: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_8 = None
        lt_8: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_8 = None
        eq_9: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_9 = None
        lt_9: "Sym(s0 < 0)" = l_self_streaming_state_offset < 0;  lt_9 = None
        eq_10: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_10 = None
        lt_10: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_10 = None
        eq_11: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_11 = None
        lt_11: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_11 = None
        eq_12: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_12 = None
        lt_12: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_12 = None
        eq_13: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_13 = None
        lt_13: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_13 = None
        eq_14: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_14 = None
        lt_14: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_14 = None
        eq_15: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_15 = None
        lt_15: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  lt_15 = None
        eq_16: "Sym(Eq(s0, 0))" = l_self_streaming_state_offset == 0;  eq_16 = None
        lt_16: "Sym(s0 < 1)" = l_self_streaming_state_offset < 1;  l_self_streaming_state_offset = lt_16 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:119 in step, code: input_ = state.cache[:, :, position : position + 1]
        add_16: "Sym((PythonMod(s0, 21)) + 1)" = mod_8 + 1
        input_: "i64[1, 17, 1][357, 21, 1]cuda:0" = l_self_streaming_state_cache[(slice(None, None, None), slice(None, None, None), slice(mod_8, add_16, None))];  l_self_streaming_state_cache = mod_8 = add_16 = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:120 in step, code: input_ = input_.permute(0, 2, 1).contiguous()
        permute: "i64[1, 1, 17][357, 1, 21]cuda:0" = input_.permute(0, 2, 1);  input_ = None
        input__1: "i64[1, 1, 17][17, 17, 1]cuda:0" = permute.contiguous();  permute = None
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:122 in step, code: 'input_ids': input_[:, :, 0],
        getitem_9: "i64[1, 1][17, 17]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), 0)]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:123 in step, code: 'speak_ids': input_[:, :, 1:9],
        getitem_10: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(1, 9, None))]
        
         # File: /share/project/hcr/flm-audio/flmaudio/models/streaming_flmaudio.py:124 in step, code: 'listen_ids': input_[:, :, 9:],
        getitem_11: "i64[1, 1, 8][17, 17, 1]cuda:0" = input__1[(slice(None, None, None), slice(None, None, None), slice(9, None, None))];  input__1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_9, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq_17 = getitem_9 == None;  getitem_9 = eq_17 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_12: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_12, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_12 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_13: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_13, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_13 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_14: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_17: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_14 += add_17;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_14;  getitem_14 = add_17 = None
        embeddings[-1] = iadd;  setitem_8 = embeddings;  iadd = setitem_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_15: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_15, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_15 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_16: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_16, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_16 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_17: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_18: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_17 += add_18;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_17;  getitem_17 = add_18 = None
        embeddings[-1] = iadd_1;  setitem_9 = embeddings;  iadd_1 = setitem_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_18: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_18, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_18 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_19: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_19, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_19 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_20: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_19: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_20 += add_19;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_20;  getitem_20 = add_19 = None
        embeddings[-1] = iadd_2;  setitem_10 = embeddings;  iadd_2 = setitem_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_21: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_21, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_21 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_22: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_22, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_22 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_23: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_20: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_23 += add_20;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_23;  getitem_23 = add_20 = None
        embeddings[-1] = iadd_3;  setitem_11 = embeddings;  iadd_3 = setitem_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_24: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_24, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_24 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_25: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_25, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_25 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_26: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_21: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_26 += add_21;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_26;  getitem_26 = add_21 = None
        embeddings[-1] = iadd_4;  setitem_12 = embeddings;  iadd_4 = setitem_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_27: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_27, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_27 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_28: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_28, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_28 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_29: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_22: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_29 += add_22;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_29;  getitem_29 = add_22 = None
        embeddings[-1] = iadd_5;  setitem_13 = embeddings;  iadd_5 = setitem_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_30: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_30, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_30 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_31: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_31, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_31 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_32: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_23: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_32 += add_23;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_32;  getitem_32 = add_23 = None
        embeddings[-1] = iadd_6;  setitem_14 = embeddings;  iadd_6 = setitem_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_33: "i64[1, 1][17, 17]cuda:0" = getitem_10[(Ellipsis, 7)];  getitem_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_33, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_33 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_34: "i64[1, 1][17, 17]cuda:0" = getitem_11[(Ellipsis, 7)];  getitem_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_34, l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_34 = l_self_modules_lm_model_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_35: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_24: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_35 += add_24;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_35;  getitem_35 = add_24 = None
        embeddings[-1] = iadd_7;  setitem_15 = embeddings;  iadd_7 = setitem_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(1, 2, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_36: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_18: "b8[][]cuda:0" = getitem_36 == 0;  getitem_36 = eq_18 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_listen_ids_: "i64[1, 1, 8][17, 17, 1]cuda:0", L_input_ids_: "i64[1, 1][17, 17]cuda:0", L_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_: "bf16[151668, 3584][3584, 1]cuda:0", L_speak_ids_: "i64[1, 1, 8][17, 17, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_: "bf16[2050, 3584][3584, 1]cuda:0", L_past_key_values_0_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_0_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_1_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_1_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_2_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_2_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_3_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_3_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_4_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_4_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_5_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_5_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_6_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_6_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_7_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_7_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_8_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_8_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_9_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_9_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_10_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_10_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_11_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_11_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_12_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_12_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_13_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_13_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_14_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_14_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_15_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_15_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_16_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_16_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_17_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_17_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_18_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_18_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_19_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_19_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_20_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_20_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_21_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_21_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_22_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_22_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_23_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_23_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_24_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_24_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_25_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_25_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_26_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_26_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0", L_past_key_values_27_0_: "bf16[1, 4, 1, 128][512, 128, 128, 1]cuda:0", L_past_key_values_27_1_: "bf16[1, 4, 1, 128][512, 128, 512, 1]cuda:0"):
        l_listen_ids_ = L_listen_ids_
        l_input_ids_ = L_input_ids_
        l_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_
        l_speak_ids_ = L_speak_ids_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_
        l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = L_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_
        l_past_key_values_0_0_ = L_past_key_values_0_0_
        l_past_key_values_0_1_ = L_past_key_values_0_1_
        l_past_key_values_1_0_ = L_past_key_values_1_0_
        l_past_key_values_1_1_ = L_past_key_values_1_1_
        l_past_key_values_2_0_ = L_past_key_values_2_0_
        l_past_key_values_2_1_ = L_past_key_values_2_1_
        l_past_key_values_3_0_ = L_past_key_values_3_0_
        l_past_key_values_3_1_ = L_past_key_values_3_1_
        l_past_key_values_4_0_ = L_past_key_values_4_0_
        l_past_key_values_4_1_ = L_past_key_values_4_1_
        l_past_key_values_5_0_ = L_past_key_values_5_0_
        l_past_key_values_5_1_ = L_past_key_values_5_1_
        l_past_key_values_6_0_ = L_past_key_values_6_0_
        l_past_key_values_6_1_ = L_past_key_values_6_1_
        l_past_key_values_7_0_ = L_past_key_values_7_0_
        l_past_key_values_7_1_ = L_past_key_values_7_1_
        l_past_key_values_8_0_ = L_past_key_values_8_0_
        l_past_key_values_8_1_ = L_past_key_values_8_1_
        l_past_key_values_9_0_ = L_past_key_values_9_0_
        l_past_key_values_9_1_ = L_past_key_values_9_1_
        l_past_key_values_10_0_ = L_past_key_values_10_0_
        l_past_key_values_10_1_ = L_past_key_values_10_1_
        l_past_key_values_11_0_ = L_past_key_values_11_0_
        l_past_key_values_11_1_ = L_past_key_values_11_1_
        l_past_key_values_12_0_ = L_past_key_values_12_0_
        l_past_key_values_12_1_ = L_past_key_values_12_1_
        l_past_key_values_13_0_ = L_past_key_values_13_0_
        l_past_key_values_13_1_ = L_past_key_values_13_1_
        l_past_key_values_14_0_ = L_past_key_values_14_0_
        l_past_key_values_14_1_ = L_past_key_values_14_1_
        l_past_key_values_15_0_ = L_past_key_values_15_0_
        l_past_key_values_15_1_ = L_past_key_values_15_1_
        l_past_key_values_16_0_ = L_past_key_values_16_0_
        l_past_key_values_16_1_ = L_past_key_values_16_1_
        l_past_key_values_17_0_ = L_past_key_values_17_0_
        l_past_key_values_17_1_ = L_past_key_values_17_1_
        l_past_key_values_18_0_ = L_past_key_values_18_0_
        l_past_key_values_18_1_ = L_past_key_values_18_1_
        l_past_key_values_19_0_ = L_past_key_values_19_0_
        l_past_key_values_19_1_ = L_past_key_values_19_1_
        l_past_key_values_20_0_ = L_past_key_values_20_0_
        l_past_key_values_20_1_ = L_past_key_values_20_1_
        l_past_key_values_21_0_ = L_past_key_values_21_0_
        l_past_key_values_21_1_ = L_past_key_values_21_1_
        l_past_key_values_22_0_ = L_past_key_values_22_0_
        l_past_key_values_22_1_ = L_past_key_values_22_1_
        l_past_key_values_23_0_ = L_past_key_values_23_0_
        l_past_key_values_23_1_ = L_past_key_values_23_1_
        l_past_key_values_24_0_ = L_past_key_values_24_0_
        l_past_key_values_24_1_ = L_past_key_values_24_1_
        l_past_key_values_25_0_ = L_past_key_values_25_0_
        l_past_key_values_25_1_ = L_past_key_values_25_1_
        l_past_key_values_26_0_ = L_past_key_values_26_0_
        l_past_key_values_26_1_ = L_past_key_values_26_1_
        l_past_key_values_27_0_ = L_past_key_values_27_0_
        l_past_key_values_27_1_ = L_past_key_values_27_1_
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:907 in forward, code: embeddings = self.text_embeddings(text_ids)
        embeddings: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_, None, None, 2.0, False, False);  l_self_modules_model_modules_embed_tokens_modules_text_embeddings_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:908 in forward, code: mask = ~(text_ids == self.config.pad_token_id)
        eq = l_input_ids_ == None;  l_input_ids_ = eq = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_1.squeeze(0);  embedding_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_1: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 0)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_2: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_1, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_, None, None, 2.0, False, False);  getitem_1 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_0_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed: "bf16[1, 3584][3584, 1]cuda:0" = embedding_2.squeeze(0);  embedding_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_2: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed + aud_listen_embed;  aud_speak_embed = aud_listen_embed = None
        getitem_2 += add;  iadd: "bf16[1, 3584][3584, 1]cuda:0" = getitem_2;  getitem_2 = add = None
        embeddings[-1] = iadd;  setitem = embeddings;  iadd = setitem = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_3: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_3: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_3, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_3 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_3.squeeze(0);  embedding_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_4: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 1)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_4: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_4, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_, None, None, 2.0, False, False);  getitem_4 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_1_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_1: "bf16[1, 3584][3584, 1]cuda:0" = embedding_4.squeeze(0);  embedding_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_5: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_1: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_1 + aud_listen_embed_1;  aud_speak_embed_1 = aud_listen_embed_1 = None
        getitem_5 += add_1;  iadd_1: "bf16[1, 3584][3584, 1]cuda:0" = getitem_5;  getitem_5 = add_1 = None
        embeddings[-1] = iadd_1;  setitem_1 = embeddings;  iadd_1 = setitem_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_6: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_5: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_6, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_6 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_5.squeeze(0);  embedding_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_7: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 2)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_6: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_7, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_, None, None, 2.0, False, False);  getitem_7 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_2_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_2: "bf16[1, 3584][3584, 1]cuda:0" = embedding_6.squeeze(0);  embedding_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_8: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_2: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_2 + aud_listen_embed_2;  aud_speak_embed_2 = aud_listen_embed_2 = None
        getitem_8 += add_2;  iadd_2: "bf16[1, 3584][3584, 1]cuda:0" = getitem_8;  getitem_8 = add_2 = None
        embeddings[-1] = iadd_2;  setitem_2 = embeddings;  iadd_2 = setitem_2 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_9: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_7: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_9, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_9 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_7.squeeze(0);  embedding_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_10: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 3)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_8: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_10, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_, None, None, 2.0, False, False);  getitem_10 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_3_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_3: "bf16[1, 3584][3584, 1]cuda:0" = embedding_8.squeeze(0);  embedding_8 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_11: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_3: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_3 + aud_listen_embed_3;  aud_speak_embed_3 = aud_listen_embed_3 = None
        getitem_11 += add_3;  iadd_3: "bf16[1, 3584][3584, 1]cuda:0" = getitem_11;  getitem_11 = add_3 = None
        embeddings[-1] = iadd_3;  setitem_3 = embeddings;  iadd_3 = setitem_3 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_12: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_9: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_12, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_12 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_9.squeeze(0);  embedding_9 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_13: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 4)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_10: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_13, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_, None, None, 2.0, False, False);  getitem_13 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_4_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_4: "bf16[1, 3584][3584, 1]cuda:0" = embedding_10.squeeze(0);  embedding_10 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_14: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_4: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_4 + aud_listen_embed_4;  aud_speak_embed_4 = aud_listen_embed_4 = None
        getitem_14 += add_4;  iadd_4: "bf16[1, 3584][3584, 1]cuda:0" = getitem_14;  getitem_14 = add_4 = None
        embeddings[-1] = iadd_4;  setitem_4 = embeddings;  iadd_4 = setitem_4 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_15: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_11: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_15, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_15 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_11.squeeze(0);  embedding_11 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_16: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 5)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_12: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_16, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_, None, None, 2.0, False, False);  getitem_16 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_5_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_5: "bf16[1, 3584][3584, 1]cuda:0" = embedding_12.squeeze(0);  embedding_12 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_17: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_5: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_5 + aud_listen_embed_5;  aud_speak_embed_5 = aud_listen_embed_5 = None
        getitem_17 += add_5;  iadd_5: "bf16[1, 3584][3584, 1]cuda:0" = getitem_17;  getitem_17 = add_5 = None
        embeddings[-1] = iadd_5;  setitem_5 = embeddings;  iadd_5 = setitem_5 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_18: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_13: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_18, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_18 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_13.squeeze(0);  embedding_13 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_19: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 6)]
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_14: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_19, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_, None, None, 2.0, False, False);  getitem_19 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_6_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_6: "bf16[1, 3584][3584, 1]cuda:0" = embedding_14.squeeze(0);  embedding_14 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_20: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_6: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_6 + aud_listen_embed_6;  aud_speak_embed_6 = aud_listen_embed_6 = None
        getitem_20 += add_6;  iadd_6: "bf16[1, 3584][3584, 1]cuda:0" = getitem_20;  getitem_20 = add_6 = None
        embeddings[-1] = iadd_6;  setitem_6 = embeddings;  iadd_6 = setitem_6 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:912 in forward, code: speak_ids[..., aud_chn_idx]
        getitem_21: "i64[1, 1][17, 17]cuda:0" = l_speak_ids_[(Ellipsis, 7)];  l_speak_ids_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:911 in forward, code: aud_speak_embed = self.aud_speak_embeddings[aud_chn_idx](
        embedding_15: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_21, l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_21 = l_self_modules_model_modules_embed_tokens_modules_aud_speak_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:913 in forward, code: ).squeeze(0)
        aud_speak_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_15.squeeze(0);  embedding_15 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:915 in forward, code: listen_ids[..., aud_chn_idx]
        getitem_22: "i64[1, 1][17, 17]cuda:0" = l_listen_ids_[(Ellipsis, 7)];  l_listen_ids_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:914 in forward, code: aud_listen_embed = self.aud_listen_embeddings[aud_chn_idx](
        embedding_16: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = torch.nn.functional.embedding(getitem_22, l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_, None, None, 2.0, False, False);  getitem_22 = l_self_modules_model_modules_embed_tokens_modules_aud_listen_embeddings_modules_7_parameters_weight_ = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:916 in forward, code: ).squeeze(0)
        aud_listen_embed_7: "bf16[1, 3584][3584, 1]cuda:0" = embedding_16.squeeze(0);  embedding_16 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:917 in forward, code: embeddings[mask] += aud_speak_embed + aud_listen_embed
        getitem_23: "bf16[1, 3584][3584, 1]cuda:0" = embeddings[-1]
        add_7: "bf16[1, 3584][3584, 1]cuda:0" = aud_speak_embed_7 + aud_listen_embed_7;  aud_speak_embed_7 = aud_listen_embed_7 = None
        getitem_23 += add_7;  iadd_7: "bf16[1, 3584][3584, 1]cuda:0" = getitem_23;  getitem_23 = add_7 = None
        embeddings[-1] = iadd_7;  setitem_7 = embeddings;  iadd_7 = setitem_7 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:920 in forward, code: embeddings = embeddings * self.input_mult
        embeddings_1: "bf16[1, 1, 3584][3584, 3584, 1]cuda:0" = embeddings * 1.0;  embeddings = embeddings_1 = None
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1130 in forward, code: cache_position = torch.arange(
        cache_position: "i64[1][1]cuda:0" = torch.arange(1, 2, device = device(type='cuda', index=0))
        
         # File: /root/.cache/huggingface/modules/transformers_modules/FLM-Audio/modeling_flmaudio.py:1140 in forward, code: (cache_position is not None and cache_position[0] == 0)
        getitem_24: "i64[][]cuda:0" = cache_position[0];  cache_position = None
        eq_1: "b8[][]cuda:0" = getitem_24 == 0;  getitem_24 = eq_1 = None
        
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7ef9f234acb0>
Traceback (most recent call last):
  File "/root/miniconda3/envs/ernie/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 113, in shutdown_compile_workers
    pool.shutdown()
  File "/root/miniconda3/envs/ernie/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 239, in shutdown
    self.process.wait(300)
  File "/root/miniconda3/envs/ernie/lib/python3.10/subprocess.py", line 1222, in wait
    self._wait(timeout=sigint_timeout)
  File "/root/miniconda3/envs/ernie/lib/python3.10/subprocess.py", line 1953, in _wait
    time.sleep(delay)
KeyboardInterrupt: 
